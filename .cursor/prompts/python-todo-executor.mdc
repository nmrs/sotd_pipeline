---
description:
globs:
alwaysApply: false
---
### python-todo-executor

**Goal:**  
Work through `TEST_SUITE_TODO.md` one task at a time. For each task: ensure a clean git state, record hashes, run the extraction pipeline, back up the 10 JSON outputs, implement the fix, re-run, compare outputs (ignoring only the `metadata` section), and either stop for user direction on differences or commit and proceed if identical.

**‚ö†Ô∏è CRITICAL WARNING: FILE SIZE AND RECORD COUNT COMPARISON ARE INSUFFICIENT**
**File size comparison alone will miss content differences and lead to false positives.**
**Record count comparison alone will miss content differences and lead to false positives.**
**ONLY full JSON content comparison (ignoring metadata only) is sufficient to validate pipeline outputs.**
**The actual content must be byte-for-byte identical outside metadata sections.**

---

## Guardrails

1. **Require clean git status**
   - Run: `git status --porcelain`
   - If any output: **stop** and show user the uncommitted changes. User must commit/stash first.

2. **Record starting commit hash**
   - `START_HASH=$(git rev-parse --verify HEAD)`
   - Append at the top of `TEST_SUITE_TODO.md`:

       <!--
       RUN_METADATA:
         start_commit: <START_HASH>
         started_at: <UTC ISO8601>
       -->

   - Do not overwrite previous values; if present, append a new `run_<N>_start_commit`.

---

## Files to Back Up / Compare

**IMPORTANT**: Multiple phases produce files with the same names (e.g., `2025-06.json`), so copy with unique prefixes to avoid overwrites.

**Required files to backup:**
- `data/extracted/2025-06.json` ‚Üí `extracted_2025-06.json`
- `data/extracted/2025-07.json` ‚Üí `extracted_2025-07.json`
- `data/matched/2025-06.json` ‚Üí `matched_2025-06.json`
- `data/matched/2025-07.json` ‚Üí `matched_2025-07.json`
- `data/enriched/2025-06.json` ‚Üí `enriched_2025-06.json`
- `data/enriched/2025-07.json` ‚Üí `enriched_2025-07.json`
- `data/aggregated/2025-06.json` ‚Üí `aggregated_2025-06.json`
- `data/aggregated/2025-07.json` ‚Üí `aggregated_2025-07.json`
- `data/reports/2025-06-hardware.md` ‚Üí `report_2025-06-hardware.md`
- `data/reports/2025-06-software.md` ‚Üí `report_2025-06-software.md`
- `data/reports/2025-07-hardware.md` ‚Üí `report_2025-07-hardware.md`
- `data/reports/2025-07-software.md` ‚Üí `report_2025-07-software.md`

**Total**: 12 files (8 JSON + 4 MD)

**Backup directory structure:**
- `.ab_backups/baseline_<timestamp>/` - Initial baseline snapshot
- `.ab_backups/task_<index>_<slug>_<timestamp>/` - Per-task snapshots

---

## Comparison Rules

**‚ö†Ô∏è MANDATORY: Full JSON Content Comparison Required**

- **Parse each JSON file** using proper JSON tools (not file system tools)
- **Ignore the entire top-level key/section `"metadata"`** (and its contents)
- **All other keys and values must be byte-for-byte identical** between baseline and post-fix snapshots
- **If `"metadata"` is missing in one file and present in the other, strip it before comparing**
- **File size comparison (`ls -la`, `wc -c`) is INSUFFICIENT and will miss content differences**
- **Use JSON parsing and comparison tools to ensure accurate validation**

**Note**: When comparing files, use the unique names from the backup directories:
- `extracted_2025-06.json` vs `extracted_2025-06.json`
- `matched_2025-06.json` vs `matched_2025-06.json`
- etc.

---

## Execution Plan

### A. Baseline

1. Run baseline extraction:

       python run.py extract:report --range 2025-06:2025-07 --force

2. Copy the 12 files into `.ab_backups/baseline_<timestamp>/` with unique names:

       cp data/extracted/2025-06.json .ab_backups/baseline_<timestamp>/extracted_2025-06.json
       cp data/extracted/2025-07.json .ab_backups/baseline_<timestamp>/extracted_2025-07.json
       cp data/matched/2025-06.json .ab_backups/baseline_<timestamp>/matched_2025-06.json
       cp data/matched/2025-07.json .ab_backups/baseline_<timestamp>/matched_2025-07.json
       cp data/enriched/2025-06.json .ab_backups/baseline_<timestamp>/enriched_2025-06.json
       cp data/enriched/2025-07.json .ab_backups/baseline_<timestamp>/enriched_2025-07.json
       cp data/aggregated/2025-06.json .ab_backups/baseline_<timestamp>/aggregated_2025-06.json
       cp data/aggregated/2025-07.json .ab_backups/baseline_<timestamp>/aggregated_2025-07.json
       cp data/reports/2025-06-hardware.md .ab_backups/baseline_<timestamp>/report_2025-06-hardware.md
       cp data/reports/2025-06-software.md .ab_backups/baseline_<timestamp>/report_2025-06-software.md
       cp data/reports/2025-07-hardware.md .ab_backups/baseline_<timestamp>/report_2025-07-hardware.md
       cp data/reports/2025-07-software.md .ab_backups/baseline_<timestamp>/report_2025-07-software.md

3. Confirm snapshot to user.

---

### B. Task Loop

**‚ö†Ô∏è CRITICAL: Every Task Requires the COMPLETE Pipeline Re-Run and Validation Process**

**DO NOT skip the pipeline re-run and comparison for any task.**
**This is a safety mechanism to catch unintended side effects.**
**Even small code changes can affect pipeline output.**

---

## **üìã MANDATORY TASK EXECUTION CHECKLIST**

**You MUST complete ALL steps below for EVERY task. NO EXCEPTIONS.**

For each unchecked task in `TEST_SUITE_TODO.md`:

### **STEP 1: Annotate Task** ‚úÖ
   - Add `start_hash: <HEAD>` under task notes.
   - **CONFIRM**: Task has start_hash annotation

### **STEP 2: Implement Fix** ‚úÖ
   - Apply minimal changes for this task.
   - **CONFIRM**: Code changes implemented and tests pass

### **STEP 3: MANDATORY Pipeline Re-Run** ‚úÖ
   - **ALWAYS** run: `python run.py extract:report --range 2025-06:2025-07 --force`
   - This validates that your fix doesn't break the pipeline or change other outputs
   - **NEVER skip this step, even for simple fixes**
   - **CONFIRM**: Pipeline completed successfully

### **STEP 4: MANDATORY Backup Post-Fix Outputs** ‚úÖ
   - Copy files into `.ab_backups/task_<index>_<slug>_<timestamp>/`
   - Use the same 12 files with unique names as baseline backup
   - **CONFIRM**: All 12 files backed up successfully

### **STEP 5: MANDATORY Baseline Comparison** ‚úÖ
   - **‚ö†Ô∏è CRITICAL: File size and record count comparison are INSUFFICIENTLY DETERMINISTIC**
   - **REQUIRED: Full JSON content comparison (ignoring `"metadata"` sections only)**
   - **Method**: Use proper JSON comparison tools, not just `ls -la`, `wc -c`, or record counting
   - **Expected result**: **BYTE-FOR-BYTE IDENTICAL content outside metadata** (your fix should not change any other data)
   - **This comparison is the safety check - don't skip it**
   - **Why content comparison matters**: 
     - File sizes can be identical while content differs
     - Record counts can be identical while content differs
     - Metadata changes can mask content changes
     - **ONLY full content comparison ensures no regressions**

   **Comparison Methodology (REQUIRED)**:
   ```bash
   # 1. Load both files and compare data sections
   python -c "
   import json
   baseline = json.load(open('.ab_backups/baseline_<timestamp>/<file>.json'))
   current = json.load(open('.ab_backups/task_<index>_<slug>_<timestamp>/<file>.json'))
   
   # Compare data sections (ignore metadata)
   baseline_data = baseline.get('data', [])
   current_data = current.get('data', [])
   
   # Record count is informational only - NOT used for validation
   print(f'Records: {len(baseline_data)} vs {len(current_data)}')
   
   # CRITICAL: This does FULL content comparison, not just counting
   # baseline_data == current_data compares every field of every record
   print(f'Content identical: {baseline_data == current_data}')
   
   # CRITICAL: Only content identical = True is acceptable
   # Record count matching is NOT sufficient for validation
   # File size matching is NOT sufficient for validation
   "
   
   # 2. For non-identical content, find first difference
   # 3. Analyze what changed and why  
   # 4. Determine if change is improvement or regression
   # 5. **STOP if content is not byte-for-byte identical outside metadata**
   ```

### **STEP 6: Branch Based on Comparison Results** ‚úÖ
   - **If non-identical (outside `metadata`):**  
     - Summarize differences:
       - Which files differ
       - Keypaths and value changes
       - Example snippets  
     - **STOP and wait for user direction.**
   - **If identical:**  
     - Check off task.  
     - Commit (`git add -A && git commit -m "Close TODO: <task title>"`).  
     - Append `resolved_by_commit: <hash>` to task notes.  
     - **Continue to next task** (only after successful validation and commit)

---

## **üéØ TASK COMPLETION VALIDATION**

**BEFORE marking any task complete, you MUST confirm:**

- [ ] **STEP 1**: Task annotated with start_hash ‚úÖ
- [ ] **STEP 2**: Fix implemented and tests pass ‚úÖ  
- [ ] **STEP 3**: Pipeline re-run completed successfully ‚úÖ
- [ ] **STEP 4**: Post-fix outputs backed up ‚úÖ
- [ ] **STEP 5**: Baseline comparison shows identical content ‚úÖ
- [ ] **STEP 6**: Task marked complete in TEST_SUITE_TODO.md ‚úÖ

**ONLY when ALL 6 steps are confirmed can you proceed to the next task.**

---

## End Condition

- All tasks checked off, or halted awaiting user input.  
- `TEST_SUITE_TODO.md` shows global `start_commit` and per-task `start_hash` / `resolved_by_commit`.  
- `.ab_backups/` contains a baseline and per-task snapshots for reproducibility.

---

## ‚ö†Ô∏è REMINDER: Validation is NOT Optional

**Every single task must complete the full validation cycle:**
1. Fix the code
2. Re-run the pipeline  
3. Backup the outputs
4. Compare against baseline
5. Only continue if identical

**This is the core safety mechanism of the script.**
**Skipping validation defeats the entire purpose.**

---

## **üö® FINAL REMINDER**

**At the end of EVERY task, ask yourself:**
- "Did I complete ALL 6 steps in the checklist?"
- "Did I validate the pipeline output against baseline?"
- "Am I sure there are no regressions?"

**If you answered NO to any question, STOP and complete the missing steps.**
**The validation loop is NOT optional - it's the entire point of this script.**

---

## Troubleshooting

### Common Issues

1. **File count mismatch**: Ensure you're copying 12 files, not 10. Multiple phases produce files with the same names.

2. **Files overwriting each other**: Use unique prefixes when copying:
   - `extracted_2025-06.json` (not just `2025-06.json`)
   - `matched_2025-06.json` (not just `2025-06.json`)
   - etc.

3. **Missing files**: Verify all required files exist before copying:
   ```bash
   ls -la data/extracted/2025-06.json data/extracted/2025-07.json data/matched/2025-06.json data/matched/2025-07.json data/enriched/2025-06.json data/enriched/2025-07.json data/aggregated/2025-06.json data/aggregated/2025-07.json data/reports/2025-06-hardware.md data/reports/2025-06-software.md data/reports/2025-07-hardware.md data/reports/2025-07-software.md
   ```

4. **Backup verification**: Always verify backup count matches expected:
   ```bash
   ls -la .ab_backups/baseline_<timestamp>/ | grep -E "\.(json|md)$" | wc -l
   # Should return 12
   ```

5. **Comparison mistakes**: Common errors that lead to false positives:
   - **File size only**: `ls -la` or `wc -c` comparison is insufficient
   - **Record count only**: `len(data)` comparison is insufficient  
   - **Metadata inclusion**: Comparing full JSON including metadata sections
   - **String comparison**: Using `diff` on raw JSON files without parsing
   - **Partial comparison**: Only checking some files or some sections
   - **Improper tools**: Using file system tools instead of JSON parsing tools
   - **Length validation**: Assuming same record count means same content
   - **Example**: 100 records with different content = same count, different data
# ML Matching Enhancement Plan

## Overview

This plan implements machine learning-based matching enhancement for the SOTD pipeline to improve match rates and handle edge cases that regex patterns struggle with. The approach uses a hybrid system that gradually transitions from regex-first to ML-first as model performance improves.

## Current State Analysis

### Existing Matching System
- **Primary**: Regex-based matching with YAML catalogs
- **Fallback**: Brand/alias matching strategies
- **Manual Overrides**: `correct_matches.yaml` for verified matches
- **Analysis Tools**: Comprehensive tools for identifying unmatched products

### Current Limitations
- Regex patterns struggle with typos and variations
- Manual pattern maintenance is time-consuming
- Limited semantic understanding of product relationships
- No learning from user corrections

## Target Architecture

### Hybrid ML + Regex System
```
Input Text → Exact Matches → Regex Patterns → ML Models → Fuzzy Fallback
     ↓           ↓              ↓            ↓           ↓
  Fastest    Manual Override  Catalog     Semantic    String
  (O(1))       (Verified)    Patterns    Similarity   Similarity
```

### ML Model Stack
1. **Sentence Transformers**: Semantic similarity (80MB, CPU-friendly)
2. **FuzzyWuzzy/RapidFuzz**: String similarity (minimal overhead)
3. **Custom NER**: Named entity recognition for brand/model extraction
4. **Ensemble Scoring**: Combine multiple models for confidence

## Implementation Phases

### Phase 1: Foundation and Infrastructure
**Goal**: Set up ML infrastructure without changing existing matching logic

#### Chunk 1.1: ML Dependencies and Base Classes
**Tasks**:
- [ ] Add ML dependencies to requirements.txt (sentence-transformers, rapidfuzz, spacy)
- [ ] Create `sotd/ml/` directory structure
- [ ] Implement `BaseMLMatcher` abstract class
- [ ] Add ML configuration to pipeline settings
- [ ] Create ML model registry and factory pattern

**Test Requirements**:
- [ ] Test ML dependency installation and imports
- [ ] Test BaseMLMatcher interface compliance
- [ ] Test configuration loading and validation
- [ ] Test model registry functionality

**Acceptance Criteria**:
- All ML dependencies install cleanly
- BaseMLMatcher provides consistent interface
- Configuration system supports ML settings
- Model registry can load and manage different ML models

#### Chunk 1.2: Semantic Similarity Matcher
**Tasks**:
- [ ] Implement `SemanticMatcher` using sentence-transformers
- [ ] Create catalog embedding generation system
- [ ] Implement similarity scoring and thresholding
- [ ] Add caching for embeddings and similarity scores
- [ ] Create training data generation from unmatched products

**Test Requirements**:
- [ ] Test embedding generation for catalog entries
- [ ] Test similarity scoring accuracy
- [ ] Test caching performance and memory usage
- [ ] Test training data generation from unmatched products
- [ ] Test threshold tuning and confidence scoring

**Acceptance Criteria**:
- Semantic matcher can generate embeddings for all catalog entries
- Similarity scores are consistent and meaningful
- Caching reduces computation time by 80%+
- Training data generation produces valid examples

#### Chunk 1.3: Fuzzy String Matcher
**Tasks**:
- [ ] Implement `FuzzyMatcher` using rapidfuzz
- [ ] Create string normalization and preprocessing
- [ ] Implement multiple fuzzy algorithms (token_sort, token_set, ratio)
- [ ] Add confidence scoring based on algorithm agreement
- [ ] Create fuzzy matching test suite

**Test Requirements**:
- [ ] Test fuzzy matching accuracy on known variations
- [ ] Test string normalization edge cases
- [ ] Test algorithm combination and scoring
- [ ] Test performance with large catalogs
- [ ] Test confidence scoring accuracy

**Acceptance Criteria**:
- Fuzzy matcher handles common typos and variations
- String normalization improves match rates
- Algorithm combination provides better confidence
- Performance remains acceptable with large catalogs

### Phase 2: Integration and Hybrid System
**Goal**: Integrate ML models into existing matching pipeline

#### Chunk 2.1: Hybrid Matcher Implementation
**Tasks**:
- [ ] Implement `HybridMatcher` that combines regex and ML
- [ ] Create confidence scoring system for all matchers
- [ ] Implement fallback chain: exact → regex → semantic → fuzzy
- [ ] Add match type tracking for ML-based matches
- [ ] Create performance monitoring for ML models

**Test Requirements**:
- [ ] Test hybrid matcher with existing test data
- [ ] Test confidence scoring accuracy
- [ ] Test fallback chain behavior
- [ ] Test performance monitoring and metrics
- [ ] Test match type consistency

**Acceptance Criteria**:
- Hybrid matcher maintains backward compatibility
- Confidence scores are meaningful and consistent
- Fallback chain works correctly in all scenarios
- Performance monitoring provides actionable insights

#### Chunk 2.2: Base Matcher Integration
**Tasks**:
- [ ] Modify `BaseMatcher` to support ML fallback
- [ ] Add ML matcher configuration to existing matchers
- [ ] Implement graceful degradation when ML models fail
- [ ] Add ML match type constants and documentation
- [ ] Update match result structure to include ML confidence

**Test Requirements**:
- [ ] Test BaseMatcher with ML fallback enabled/disabled
- [ ] Test graceful degradation scenarios
- [ ] Test match result structure consistency
- [ ] Test configuration validation
- [ ] Test backward compatibility

**Acceptance Criteria**:
- BaseMatcher works with and without ML fallback
- Graceful degradation prevents pipeline failures
- Match result structure is consistent
- Configuration system is robust

#### Chunk 2.3: Product-Specific ML Matchers
**Tasks**:
- [ ] Implement `MLRazorMatcher` with razor-specific optimizations
- [ ] Implement `MLBladeMatcher` with blade-specific patterns
- [ ] Implement `MLBrushMatcher` with brush-specific features
- [ ] Implement `MLSoapMatcher` with soap-specific semantics
- [ ] Create product-specific training data generators

**Test Requirements**:
- [ ] Test each product-specific matcher with relevant data
- [ ] Test product-specific optimizations and features
- [ ] Test training data generation for each product type
- [ ] Test cross-product contamination prevention
- [ ] Test product-specific confidence scoring

**Acceptance Criteria**:
- Each product matcher improves on generic ML matching
- Product-specific features provide meaningful improvements
- Training data generation is product-aware
- Cross-product contamination is prevented

### Phase 3: Training and Improvement System
**Goal**: Create system for continuous ML model improvement

#### Chunk 3.1: Training Data Pipeline
**Tasks**:
- [ ] Create `TrainingDataGenerator` from unmatched products
- [ ] Implement manual annotation interface for training data
- [ ] Create training data validation and quality checks
- [ ] Add training data versioning and management
- [ ] Implement training data export/import functionality

**Test Requirements**:
- [ ] Test training data generation from unmatched products
- [ ] Test manual annotation interface usability
- [ ] Test training data validation accuracy
- [ ] Test versioning and management functionality
- [ ] Test export/import functionality

**Acceptance Criteria**:
- Training data generation produces high-quality examples
- Manual annotation interface is user-friendly
- Training data validation catches quality issues
- Versioning system tracks training data evolution

#### Chunk 3.2: Model Training and Evaluation
**Tasks**:
- [ ] Implement model training pipeline for sentence transformers
- [ ] Create model evaluation metrics and reporting
- [ ] Implement model versioning and A/B testing
- [ ] Add model performance monitoring and alerting
- [ ] Create model rollback and recovery procedures

**Test Requirements**:
- [ ] Test model training pipeline end-to-end
- [ ] Test evaluation metrics accuracy
- [ ] Test model versioning and A/B testing
- [ ] Test performance monitoring accuracy
- [ ] Test rollback and recovery procedures

**Acceptance Criteria**:
- Model training pipeline produces improved models
- Evaluation metrics accurately reflect model performance
- A/B testing can compare model versions
- Performance monitoring provides early warning of issues

#### Chunk 3.3: Continuous Learning System
**Tasks**:
- [ ] Implement feedback collection from user corrections
- [ ] Create automatic retraining triggers based on performance
- [ ] Implement model drift detection and alerting
- [ ] Add model performance tracking over time
- [ ] Create model improvement recommendations

**Test Requirements**:
- [ ] Test feedback collection accuracy
- [ ] Test automatic retraining trigger logic
- [ ] Test drift detection sensitivity
- [ ] Test performance tracking accuracy
- [ ] Test improvement recommendation quality

**Acceptance Criteria**:
- Feedback collection improves model performance
- Automatic retraining maintains model quality
- Drift detection provides early warning
- Performance tracking shows improvement trends

### Phase 4: Gradual Transition and Optimization
**Goal**: Gradually shift from regex-first to ML-first matching

#### Chunk 4.1: Confidence-Based Routing
**Tasks**:
- [ ] Implement confidence-based routing between regex and ML
- [ ] Create dynamic threshold adjustment based on performance
- [ ] Add ML model performance tracking and comparison
- [ ] Implement gradual ML adoption based on confidence
- [ ] Create ML model performance dashboard

**Test Requirements**:
- [ ] Test confidence-based routing accuracy
- [ ] Test dynamic threshold adjustment logic
- [ ] Test performance tracking accuracy
- [ ] Test gradual adoption logic
- [ ] Test dashboard functionality

**Acceptance Criteria**:
- Confidence-based routing improves overall match quality
- Dynamic thresholds adapt to changing performance
- Performance tracking shows clear improvement trends
- Gradual adoption prevents performance regressions

#### Chunk 4.2: ML-First Matching Implementation
**Tasks**:
- [ ] Implement ML-first matching for high-confidence cases
- [ ] Create ML model ensemble for improved accuracy
- [ ] Add ML model specialization for different product types
- [ ] Implement ML model caching and optimization
- [ ] Create ML model performance benchmarks

**Test Requirements**:
- [ ] Test ML-first matching accuracy and performance
- [ ] Test ensemble model improvement over single models
- [ ] Test model specialization effectiveness
- [ ] Test caching and optimization impact
- [ ] Test benchmark accuracy and consistency

**Acceptance Criteria**:
- ML-first matching improves overall accuracy
- Ensemble models outperform single models
- Model specialization provides meaningful improvements
- Caching and optimization maintain performance

#### Chunk 4.3: Performance Optimization and Monitoring
**Tasks**:
- [ ] Implement ML model performance optimization
- [ ] Create comprehensive ML monitoring and alerting
- [ ] Add ML model resource usage tracking
- [ ] Implement ML model scaling and load balancing
- [ ] Create ML model maintenance procedures

**Test Requirements**:
- [ ] Test performance optimization effectiveness
- [ ] Test monitoring and alerting accuracy
- [ ] Test resource usage tracking
- [ ] Test scaling and load balancing
- [ ] Test maintenance procedures

**Acceptance Criteria**:
- Performance optimization maintains accuracy while improving speed
- Monitoring provides actionable insights
- Resource usage tracking helps with capacity planning
- Scaling and load balancing handle increased load

## Testing Strategy

### Unit Testing
- Each ML matcher has comprehensive unit tests
- Mock external dependencies (sentence-transformers, spacy)
- Test edge cases and error conditions
- Test performance characteristics

### Integration Testing
- Test ML matchers with real catalog data
- Test hybrid system with existing pipeline
- Test training data pipeline end-to-end
- Test model training and evaluation

### Performance Testing
- Benchmark ML model inference times
- Test memory usage with large catalogs
- Test caching effectiveness
- Test scaling characteristics

### Quality Assurance
- Run existing test suite to ensure no regressions
- Test backward compatibility
- Test graceful degradation
- Test configuration validation

## Configuration and Settings

### ML Configuration
```yaml
ml_matching:
  enabled: true
  models:
    semantic:
      model_name: "all-MiniLM-L6-v2"
      threshold: 0.7
      cache_size: 1000
    fuzzy:
      threshold: 80
      algorithms: ["token_sort", "token_set", "ratio"]
    ner:
      model_name: "en_core_web_sm"
      threshold: 0.8
  training:
    auto_retrain: true
    retrain_threshold: 0.05
    training_data_path: "data/ml_training/"
  performance:
    enable_caching: true
    cache_ttl: 3600
    max_memory_mb: 512
```

### Gradual Transition Settings
```yaml
ml_transition:
  phase: "hybrid"  # "regex_first", "hybrid", "ml_first"
  confidence_threshold: 0.8
  adoption_rate: 0.1  # 10% of cases use ML-first
  performance_threshold: 0.95  # ML must be 95% as good as regex
```

## Success Metrics

### Match Quality Metrics
- **Match Rate**: Percentage of products successfully matched
- **Accuracy**: Percentage of correct matches (vs manual verification)
- **Confidence Distribution**: Distribution of confidence scores
- **False Positive Rate**: Percentage of incorrect matches

### Performance Metrics
- **Inference Time**: Average time per match
- **Memory Usage**: Peak memory usage during matching
- **Throughput**: Matches per second
- **Cache Hit Rate**: Percentage of cache hits

### Training Metrics
- **Training Data Quality**: Percentage of high-quality training examples
- **Model Improvement**: Improvement in accuracy over time
- **Drift Detection**: Time to detect model performance degradation
- **Retraining Frequency**: How often models need retraining

## Risk Mitigation

### Technical Risks
- **Model Performance**: Start with proven models (sentence-transformers)
- **Memory Usage**: Implement caching and resource limits
- **Inference Speed**: Use lightweight models and optimization
- **Model Drift**: Implement monitoring and retraining triggers

### Operational Risks
- **Backward Compatibility**: Maintain existing API and behavior
- **Gradual Rollout**: Use confidence-based routing for safe adoption
- **Rollback Capability**: Keep regex as fallback
- **Monitoring**: Comprehensive monitoring and alerting

### Data Risks
- **Training Data Quality**: Implement validation and quality checks
- **Data Privacy**: Use only public catalog data for training
- **Bias**: Monitor for bias in training data and model outputs
- **Versioning**: Track training data and model versions

## Implementation Prompts

### Prompt 1: Foundation Setup
```text
Implement the ML matching foundation for the SOTD pipeline. Start by:

1. Adding ML dependencies to requirements.txt:
   - sentence-transformers (for semantic similarity)
   - rapidfuzz (for fuzzy string matching)
   - spacy (for NER, with en_core_web_sm model)

2. Create the ML directory structure:
   - sotd/ml/__init__.py
   - sotd/ml/base.py (BaseMLMatcher abstract class)
   - sotd/ml/config.py (ML configuration management)
   - sotd/ml/registry.py (model registry and factory)

3. Implement BaseMLMatcher with:
   - Abstract match() method returning (match_data, confidence_score)
   - Configuration loading and validation
   - Performance monitoring hooks
   - Error handling and graceful degradation

4. Create comprehensive tests for:
   - Dependency installation and imports
   - BaseMLMatcher interface compliance
   - Configuration loading and validation
   - Model registry functionality

Focus on creating a solid foundation that can support multiple ML models while maintaining clean separation of concerns. Ensure all tests pass and the system integrates cleanly with the existing pipeline architecture.
```

### Prompt 2: Semantic Similarity Implementation
```text
Implement the semantic similarity matcher using sentence-transformers. This should:

1. Create SemanticMatcher class extending BaseMLMatcher:
   - Load sentence-transformers model (all-MiniLM-L6-v2 for speed/size balance)
   - Generate embeddings for all catalog entries
   - Implement similarity scoring with configurable thresholds
   - Add caching for embeddings and similarity scores

2. Implement catalog embedding generation:
   - Create multiple text variations for each product (brand+model, model+brand, etc.)
   - Generate embeddings for all variations
   - Store embeddings efficiently with product metadata
   - Implement embedding versioning and updates

3. Add training data generation:
   - Parse unmatched products from analysis tools
   - Generate positive examples from correct_matches.yaml
   - Create negative examples from incorrect matches
   - Implement data validation and quality checks

4. Create comprehensive tests:
   - Test embedding generation for catalog entries
   - Test similarity scoring accuracy with known examples
   - Test caching performance and memory usage
   - Test training data generation quality

Focus on performance and accuracy. The semantic matcher should handle typos and variations that regex patterns struggle with, while maintaining reasonable inference times on CPU.
```

### Prompt 3: Fuzzy String Matching
```text
Implement the fuzzy string matcher using rapidfuzz. This should:

1. Create FuzzyMatcher class extending BaseMLMatcher:
   - Implement multiple fuzzy algorithms (token_sort, token_set, ratio)
   - Add string normalization and preprocessing
   - Create confidence scoring based on algorithm agreement
   - Implement efficient catalog indexing for fast lookups

2. Add string preprocessing:
   - Normalize case, whitespace, and punctuation
   - Handle common abbreviations and variations
   - Implement product-specific preprocessing rules
   - Add preprocessing caching for performance

3. Implement algorithm combination:
   - Weight different algorithms based on product type
   - Create ensemble scoring from multiple algorithms
   - Add confidence thresholds for different algorithms
   - Implement fallback logic for edge cases

4. Create comprehensive tests:
   - Test fuzzy matching on known variations and typos
   - Test string normalization edge cases
   - Test algorithm combination and scoring
   - Test performance with large catalogs

Focus on handling the common variations and typos that appear in user input while maintaining fast performance. The fuzzy matcher should complement the semantic matcher by handling exact string variations.
```

### Prompt 4: Hybrid System Integration
```text
Implement the hybrid matcher that combines regex and ML approaches. This should:

1. Create HybridMatcher class:
   - Implement fallback chain: exact → regex → semantic → fuzzy
   - Add confidence scoring for all matchers
   - Create match type tracking for ML-based matches
   - Implement performance monitoring and metrics

2. Modify BaseMatcher integration:
   - Add ML matcher configuration to existing matchers
   - Implement graceful degradation when ML models fail
   - Add ML match type constants and documentation
   - Update match result structure to include ML confidence

3. Add confidence-based routing:
   - Route high-confidence ML matches before regex
   - Implement dynamic threshold adjustment
   - Add ML model performance tracking
   - Create gradual ML adoption based on confidence

4. Create comprehensive tests:
   - Test hybrid matcher with existing test data
   - Test confidence scoring accuracy
   - Test fallback chain behavior
   - Test graceful degradation scenarios

Focus on maintaining backward compatibility while adding ML capabilities. The hybrid system should improve match rates without breaking existing functionality.
```

### Prompt 5: Training and Improvement System
```text
Implement the training and improvement system for continuous ML model enhancement. This should:

1. Create TrainingDataGenerator:
   - Parse unmatched products from analysis tools
   - Generate training examples from correct_matches.yaml
   - Implement manual annotation interface
   - Add training data validation and quality checks

2. Implement model training pipeline:
   - Create training pipeline for sentence transformers
   - Add model evaluation metrics and reporting
   - Implement model versioning and A/B testing
   - Add model performance monitoring and alerting

3. Add continuous learning system:
   - Collect feedback from user corrections
   - Implement automatic retraining triggers
   - Add model drift detection and alerting
   - Create model improvement recommendations

4. Create comprehensive tests:
   - Test training data generation quality
   - Test model training pipeline end-to-end
   - Test feedback collection accuracy
   - Test drift detection sensitivity

Focus on creating a system that can continuously improve ML model performance based on real usage data. The training system should be automated but allow for manual oversight and intervention.
```

### Prompt 6: Gradual Transition Implementation
```text
Implement the gradual transition from regex-first to ML-first matching. This should:

1. Implement confidence-based routing:
   - Route high-confidence ML matches before regex
   - Add dynamic threshold adjustment based on performance
   - Implement ML model performance tracking and comparison
   - Create gradual ML adoption based on confidence scores

2. Add ML-first matching for high-confidence cases:
   - Implement ML model ensemble for improved accuracy
   - Add ML model specialization for different product types
   - Implement ML model caching and optimization
   - Create ML model performance benchmarks

3. Add comprehensive monitoring:
   - Implement ML model performance optimization
   - Create comprehensive ML monitoring and alerting
   - Add ML model resource usage tracking
   - Implement ML model maintenance procedures

4. Create comprehensive tests:
   - Test confidence-based routing accuracy
   - Test ML-first matching performance
   - Test monitoring and alerting accuracy
   - Test maintenance procedures

Focus on creating a smooth transition that improves overall match quality while maintaining system stability. The gradual transition should be data-driven and allow for easy rollback if issues arise.
```

## Gradual Transition Strategy

### Phase 1: ML as Fallback (Months 1-2)
- **Goal**: Add ML models as fallback for unmatched products
- **Success Metric**: 10-20% improvement in match rate
- **Risk Level**: Low (regex remains primary)

### Phase 2: Hybrid Routing (Months 3-4)
- **Goal**: Use confidence-based routing between regex and ML
- **Success Metric**: 20-30% improvement in match rate
- **Risk Level**: Medium (careful monitoring required)

### Phase 3: ML-First for High Confidence (Months 5-6)
- **Goal**: Use ML-first for high-confidence cases
- **Success Metric**: 30-40% improvement in match rate
- **Risk Level**: Medium (gradual rollout)

### Phase 4: Full ML-First (Months 7+)
- **Goal**: ML-first matching with regex as fallback
- **Success Metric**: 40%+ improvement in match rate
- **Risk Level**: High (requires proven ML performance)

## Continuous Improvement Process

### Weekly Monitoring
- Track match rates and accuracy by product type
- Monitor ML model performance and drift
- Review unmatched products for training data opportunities
- Analyze confidence score distributions

### Monthly Evaluation
- Evaluate ML model performance vs regex
- Review training data quality and quantity
- Assess need for model retraining
- Plan gradual transition adjustments

### Quarterly Review
- Comprehensive performance review
- Training data quality assessment
- Model architecture evaluation
- Transition strategy adjustment

## Success Criteria

### Short-term (3 months)
- [ ] ML models successfully integrated as fallback
- [ ] 10-20% improvement in overall match rate
- [ ] No performance degradation in existing pipeline
- [ ] Training data pipeline operational

### Medium-term (6 months)
- [ ] Hybrid routing system operational
- [ ] 20-30% improvement in overall match rate
- [ ] ML models performing at 95%+ of regex accuracy
- [ ] Continuous learning system operational

### Long-term (12 months)
- [ ] ML-first matching for high-confidence cases
- [ ] 30-40% improvement in overall match rate
- [ ] ML models consistently outperforming regex
- [ ] Fully automated training and improvement system

## Conclusion

This ML matching enhancement plan provides a comprehensive approach to improving the SOTD pipeline's matching capabilities while maintaining system stability and backward compatibility. The gradual transition strategy ensures that improvements are made incrementally and safely, with continuous monitoring and the ability to rollback if needed.

The key to success is the hybrid approach that combines the strengths of both regex and ML systems, with a data-driven transition strategy that prioritizes match quality and system reliability.
description:
globs:
alwaysApply: false
---
